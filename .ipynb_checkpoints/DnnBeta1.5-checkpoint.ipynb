{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "16b1853f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              ID    lineage\n",
      "0     \"ON414702\"  \"BA.2.31\"\n",
      "1     \"ON056475\"  \"BA.2.31\"\n",
      "2     \"ON203732\"  \"BA.2.31\"\n",
      "3     \"OW275801\"  \"BA.2.31\"\n",
      "4     \"ON914730\"  \"BA.2.31\"\n",
      "...          ...        ...\n",
      "4995  \"OM122217\"  \"BA.1.17\"\n",
      "4996  \"OV662626\"  \"BA.1.17\"\n",
      "4997  \"OM200680\"  \"BA.1.17\"\n",
      "4998  \"OW742696\"  \"BA.1.17\"\n",
      "4999  \"OW120613\"  \"BA.1.17\"\n",
      "\n",
      "[5000 rows x 2 columns]\n",
      "              ID                                       Enc_Sequence    lineage\n",
      "0     \"FR990446\"  0000010000000101000001010000011111110110001101...  \"B.1.258\"\n",
      "1     \"FR991388\"  1010111111001100010111110101010010101100000100...  \"B.1.258\"\n",
      "2     \"FR991439\"  0000010100000111111101100011011101111110110010...  \"B.1.258\"\n",
      "3     \"HG994296\"  0000000000000000000000000000000000000001010010...  \"B.1.258\"\n",
      "4     \"HG994312\"  0000000000000000000000000000000000000000000000...  \"B.1.258\"\n",
      "...          ...                                                ...        ...\n",
      "4995  \"OX273314\"  0000000000000000000000000000000000000000000000...   \"BA.5.1\"\n",
      "4996  \"OX273355\"  0000000000000000000000000000000000000000000000...   \"BA.5.1\"\n",
      "4997  \"OX273527\"  0000000000000000000000000000000000000000000000...   \"BA.5.1\"\n",
      "4998  \"OX273539\"  0000000000000000000000000000000000000000000000...   \"BA.5.1\"\n",
      "4999  \"OX273975\"  0000000000000000000000000000000000000000000000...   \"BA.5.1\"\n",
      "\n",
      "[5000 rows x 3 columns]\n",
      "ViralSequencesDataset\n",
      "Dataset size: 5000\n",
      "Max length: 59814\n",
      "\n",
      "Data:\n",
      "           ID                                       Enc_Sequence    lineage\n",
      "0  \"FR990446\"  0000010000000101000001010000011111110110001101...  \"B.1.258\"\n",
      "1  \"FR991388\"  1010111111001100010111110101010010101100000100...  \"B.1.258\"\n",
      "2  \"FR991439\"  0000010100000111111101100011011101111110110010...  \"B.1.258\"\n",
      "3  \"HG994296\"  0000000000000000000000000000000000000001010010...  \"B.1.258\"\n",
      "4  \"HG994312\"  0000000000000000000000000000000000000000000000...  \"B.1.258\"\n",
      "Using device: cuda\n",
      "Epoch [1/5] - Batch [10/63] - Loss: 2.0084 - Accuracy: 33.91%\n",
      "Epoch [1/5] - Batch [20/63] - Loss: 1.5292 - Accuracy: 43.91%\n",
      "Epoch [1/5] - Batch [30/63] - Loss: 1.2468 - Accuracy: 59.38%\n",
      "Epoch [1/5] - Batch [40/63] - Loss: 1.2167 - Accuracy: 62.34%\n",
      "Epoch [1/5] - Batch [50/63] - Loss: 1.1224 - Accuracy: 62.97%\n",
      "Epoch [1/5] - Batch [60/63] - Loss: 1.0386 - Accuracy: 64.53%\n",
      "Epoch [2/5] - Batch [10/63] - Loss: 0.9245 - Accuracy: 68.28%\n",
      "Epoch [2/5] - Batch [20/63] - Loss: 0.8893 - Accuracy: 69.22%\n",
      "Epoch [2/5] - Batch [30/63] - Loss: 0.8406 - Accuracy: 69.53%\n",
      "Epoch [2/5] - Batch [40/63] - Loss: 0.8885 - Accuracy: 69.84%\n",
      "Epoch [2/5] - Batch [50/63] - Loss: 0.8270 - Accuracy: 71.56%\n",
      "Epoch [2/5] - Batch [60/63] - Loss: 0.8055 - Accuracy: 72.19%\n",
      "Epoch [3/5] - Batch [10/63] - Loss: 0.7695 - Accuracy: 73.28%\n",
      "Epoch [3/5] - Batch [20/63] - Loss: 0.6910 - Accuracy: 73.75%\n",
      "Epoch [3/5] - Batch [30/63] - Loss: 0.6990 - Accuracy: 74.69%\n",
      "Epoch [3/5] - Batch [40/63] - Loss: 0.6574 - Accuracy: 77.81%\n",
      "Epoch [3/5] - Batch [50/63] - Loss: 0.7186 - Accuracy: 75.62%\n",
      "Epoch [3/5] - Batch [60/63] - Loss: 0.6247 - Accuracy: 76.88%\n",
      "Epoch [4/5] - Batch [10/63] - Loss: 0.6228 - Accuracy: 77.50%\n",
      "Epoch [4/5] - Batch [20/63] - Loss: 0.6001 - Accuracy: 77.97%\n",
      "Epoch [4/5] - Batch [30/63] - Loss: 0.5743 - Accuracy: 79.06%\n",
      "Epoch [4/5] - Batch [40/63] - Loss: 0.5707 - Accuracy: 79.84%\n",
      "Epoch [4/5] - Batch [50/63] - Loss: 0.6151 - Accuracy: 76.56%\n",
      "Epoch [4/5] - Batch [60/63] - Loss: 0.6207 - Accuracy: 77.50%\n",
      "Epoch [5/5] - Batch [10/63] - Loss: 0.5297 - Accuracy: 80.47%\n",
      "Epoch [5/5] - Batch [20/63] - Loss: 0.5178 - Accuracy: 82.34%\n",
      "Epoch [5/5] - Batch [30/63] - Loss: 0.5907 - Accuracy: 78.75%\n",
      "Epoch [5/5] - Batch [40/63] - Loss: 0.5608 - Accuracy: 81.25%\n",
      "Epoch [5/5] - Batch [50/63] - Loss: 0.5583 - Accuracy: 78.75%\n",
      "Epoch [5/5] - Batch [60/63] - Loss: 0.4897 - Accuracy: 84.06%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import random\n",
    "from torch.utils.data import random_split\n",
    "# Step 1: Load the data\n",
    "lineage_file = \"./dataset1.csv\"\n",
    "text_files_folder = \"./text_files\"\n",
    "\n",
    "# Step 2: Load all the text files to converted_df\n",
    "converted_data = []\n",
    "for file_path in os.listdir(text_files_folder):\n",
    "    file_name = os.path.splitext(file_path)[0]\n",
    "    with open(os.path.join(text_files_folder, file_path), \"r\") as file:\n",
    "        content = file.read().strip()\n",
    "        converted_data.append(['\"' + file_name + '\"', content])\n",
    "\n",
    "converted_df = pd.DataFrame(converted_data, columns=[\"ID\", \"Enc_Sequence\"])\n",
    "max_length = converted_df[\"Enc_Sequence\"].str.len().max()\n",
    "####################################################################################\n",
    "\n",
    "# Step 3: Load lineage file as DataFrame\n",
    "lineage_df = pd.read_csv(lineage_file)\n",
    "print(lineage_df)\n",
    "merged_df = pd.merge(converted_df, lineage_df, on=\"ID\", how=\"left\")\n",
    "converted_df[\"lineage\"] = merged_df[\"lineage\"]\n",
    "\n",
    "print(converted_df)\n",
    "\n",
    "\n",
    "lineages = lineage_df[\"lineage\"].unique()\n",
    "num_classes = len(lineages)\n",
    "lineage_to_id = {lineage: i for i, lineage in enumerate(lineages)}\n",
    "\n",
    "#####################################################################################\n",
    "\n",
    "# Step 4: Prepare the data for training\n",
    "class ViralSequencesDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, converted_df, max_length, lineage_to_id):\n",
    "        self.data = converted_df\n",
    "        self.max_length = max_length\n",
    "        self.lineage_to_id = lineage_to_id\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        sequence = self.data.iloc[index][\"Enc_Sequence\"]\n",
    "        enc_sequence = np.array(list(sequence), dtype=np.uint8)\n",
    "        enc_sequence = enc_sequence.reshape(-1)  # Remove extra dimensions\n",
    "\n",
    "        # Pad the sequence with zeros\n",
    "        padded_sequence = nn.functional.pad(torch.tensor(enc_sequence), pad=(0, self.max_length - len(enc_sequence)))\n",
    "\n",
    "        lineage = self.data.iloc[index][\"lineage\"]\n",
    "        lineage_id = self.lineage_to_id[lineage]\n",
    "\n",
    "        return padded_sequence, lineage_id, len(enc_sequence)  # Return sequence length\n",
    "    \n",
    "    def __str__(self):\n",
    "        dataset_info = f\"ViralSequencesDataset\\nDataset size: {len(self)}\\nMax length: {self.max_length}\"\n",
    "        data_info = f\"Data:\\n{self.data.head()}\"\n",
    "        return f\"{dataset_info}\\n\\n{data_info}\"\n",
    "\n",
    "\n",
    "# dataset = ViralSequencesDataset(output_file, max_length)\n",
    "dataset = ViralSequencesDataset(converted_df, max_length, lineage_to_id)\n",
    "print(dataset)\n",
    "# Set the batch size and split ratio\n",
    "batch_size = 64\n",
    "train_ratio = 0.8\n",
    "\n",
    "train_size = int(train_ratio * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "\n",
    "# Create data loaders for training and testing\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "#####################################################################################\n",
    "\n",
    "# Step 5: Define the model architecture\n",
    "class LineageClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(LineageClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc4 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x, length):\n",
    "        out = self.fc1(x.float())\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc3(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc4(out)\n",
    "        return out\n",
    "#####################################################################################\n",
    "\n",
    "# Step 6: Train the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Set the sequence length\n",
    "sequence_length = max_length\n",
    "\n",
    "# Set the number of output classes\n",
    "num_classes = 10  # Change this to the appropriate number of classes\n",
    "\n",
    "Hidden_size=512\n",
    "\n",
    "# Initialize the model\n",
    "model = LineageClassifier(input_size=sequence_length, hidden_size=Hidden_size, num_classes=num_classes)\n",
    "model = model.to(device)\n",
    "\n",
    "# Set the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# Set the number of epochs\n",
    "num_epochs = 5\n",
    "\n",
    "# Initialize variables for early stopping\n",
    "best_loss = float('inf')\n",
    "patience = 3\n",
    "counter = 0\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "\n",
    "    for batch_idx, (inputs, targets, lengths) in enumerate(train_loader):\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs, lengths)  # Pass lengths to the model\n",
    "\n",
    "        targets = targets.view(-1)\n",
    "        outputs = outputs.view(-1, num_classes)\n",
    "\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        correct_predictions += (predicted == targets).sum().item()\n",
    "\n",
    "        if batch_idx % 10 == 9:\n",
    "            batch_loss = running_loss / 10\n",
    "            accuracy = 100 * correct_predictions / (10 * len(inputs))\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}] - Batch [{batch_idx+1}/{len(train_loader)}] - Loss: {batch_loss:.4f} - Accuracy: {accuracy:.2f}%\")\n",
    "            running_loss = 0.0\n",
    "            correct_predictions = 0\n",
    "\n",
    "#####################################################################################\n",
    "\n",
    "# Step 7: Save the trained model\n",
    "torch.save(model.state_dict(), \"./lineage_classifier.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d9495eab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 76.30%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#####################\n",
    "# WORKING TEST\n",
    "#####################\n",
    "# Step 8: Test the model\n",
    "def test_model(model, test_loader, device):\n",
    "    model.eval()\n",
    "    total_samples = 0\n",
    "    correct_predictions = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets, lengths in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            outputs = model(inputs, lengths)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            total_samples += targets.size(0)\n",
    "            correct_predictions += (predicted == targets).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct_predictions / total_samples\n",
    "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "# Call the test function\n",
    "test_model(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a44ef475",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing lineages: 100%|█████████████████████████████████████████████████████████████| 10/10 [02:25<00:00, 14.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import requests\n",
    "import io\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from Bio import SeqIO\n",
    "import concurrent.futures\n",
    "\n",
    "with open(\"./data/metadata\", \"r\") as file:\n",
    "    metadata_lines = file.readlines()\n",
    "\n",
    "lineage_samples = defaultdict(list)\n",
    "unique_lineages = set()\n",
    "samples_list = []\n",
    "\n",
    "for line in metadata_lines[1:]:\n",
    "    columns = line.strip().split(\"\\t\")\n",
    "    accession_id = columns[0]\n",
    "    lineage = columns[1]\n",
    "    lineage_samples[lineage].append(accession_id)\n",
    "    unique_lineages.add(lineage)\n",
    "\n",
    "# Ensure selected lineages have at least 500 samples\n",
    "selected_lineages = [lineage for lineage in unique_lineages if len(lineage_samples[lineage]) >= 500][:10]\n",
    "\n",
    "max_sequences_per_lineage = 500\n",
    "max_sequences = max_sequences_per_lineage * len(selected_lineages)\n",
    "\n",
    "def download_sample(sample, lineage):\n",
    "    url = f\"https://www.ebi.ac.uk/Tools/dbfetch/dbfetch?db=ena_sequence&id={sample}&format=fasta&style=raw&Retrieve=Retrieve\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        filename = re.sub(r'[\\\\/:\\*\\?\"<>\\|()]', '_', sample)\n",
    "        with io.open(f\"./data/fasta_files/{filename}.fasta\", \"w\", encoding=\"utf-8\") as seq_file:\n",
    "            seq_file.write(response.text)\n",
    "        # Append the data to the list without the sequence\n",
    "        return {'id': sample, 'lineage': lineage}\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=20) as executor:\n",
    "    for lineage in tqdm(selected_lineages, desc=\"Processing lineages\"):\n",
    "        samples = random.sample(lineage_samples[lineage], max_sequences_per_lineage)\n",
    "        lineage_samples[lineage] = list(set(lineage_samples[lineage]) - set(samples))\n",
    "        futures = {executor.submit(download_sample, sample, lineage) for sample in samples}\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            result = future.result()\n",
    "            if result:\n",
    "                samples_list.append(result)\n",
    "\n",
    "print(\"Download completed!\")\n",
    "\n",
    "# Convert the list of dictionaries to a dataframe\n",
    "df = pd.DataFrame(samples_list)\n",
    "\n",
    "# Save the dataframe to csv\n",
    "df.to_csv(\"./data/dataset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6643c7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.py\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class DNA_CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DNA_CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=(4, 1))\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(4, 1))\n",
    "        self.fc1 = nn.Linear(256, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        self.max_pool = nn.MaxPool2d((2, 1))\n",
    "        self.adaptive_pool = nn.AdaptiveMaxPool2d((1, 4))\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.max_pool(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.max_pool(x)\n",
    "        x = self.adaptive_pool(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b6a97db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5000, 28821, 4])\n",
      "torch.Size([5000, 1])\n",
      "torch.Size([5000, 1, 28821, 4])\n",
      "torch.Size([5000])\n",
      "Running on CPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|█████▏                                                                            | 8/125 [00:49<11:58,  6.14s/it]\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\roeym\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3444, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\roeym\\AppData\\Local\\Temp/ipykernel_22440/223147076.py\", line 145, in <module>\n",
      "    main()\n",
      "  File \"C:\\Users\\roeym\\AppData\\Local\\Temp/ipykernel_22440/223147076.py\", line 134, in main\n",
      "    avg = train(model, criterion, optimizer, train_loader, device)\n",
      "  File \"C:\\Users\\roeym\\AppData\\Local\\Temp/ipykernel_22440/223147076.py\", line 73, in train\n",
      "    loss.backward()\n",
      "  File \"C:\\Users\\roeym\\anaconda3\\lib\\site-packages\\torch\\_tensor.py\", line 396, in backward\n",
      "    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)\n",
      "  File \"C:\\Users\\roeym\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\", line 173, in backward\n",
      "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\roeym\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2064, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\roeym\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"C:\\Users\\roeym\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 248, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\roeym\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 281, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"C:\\Users\\roeym\\anaconda3\\lib\\inspect.py\", line 1541, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"C:\\Users\\roeym\\anaconda3\\lib\\inspect.py\", line 1499, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"C:\\Users\\roeym\\anaconda3\\lib\\inspect.py\", line 709, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"C:\\Users\\roeym\\anaconda3\\lib\\inspect.py\", line 755, in getmodule\n",
      "    os.path.realpath(f)] = module.__name__\n",
      "  File \"C:\\Users\\roeym\\anaconda3\\lib\\ntpath.py\", line 664, in realpath\n",
      "    if _getfinalpathname(spath) == path:\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\roeym\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3444, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\roeym\\AppData\\Local\\Temp/ipykernel_22440/223147076.py\", line 145, in <module>\n",
      "    main()\n",
      "  File \"C:\\Users\\roeym\\AppData\\Local\\Temp/ipykernel_22440/223147076.py\", line 134, in main\n",
      "    avg = train(model, criterion, optimizer, train_loader, device)\n",
      "  File \"C:\\Users\\roeym\\AppData\\Local\\Temp/ipykernel_22440/223147076.py\", line 73, in train\n",
      "    loss.backward()\n",
      "  File \"C:\\Users\\roeym\\anaconda3\\lib\\site-packages\\torch\\_tensor.py\", line 396, in backward\n",
      "    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)\n",
      "  File \"C:\\Users\\roeym\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\", line 173, in backward\n",
      "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\roeym\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2064, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\roeym\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3364, in run_ast_nodes\n",
      "    if (await self.run_code(code, result,  async_=asy)):\n",
      "  File \"C:\\Users\\roeym\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3461, in run_code\n",
      "    self.showtraceback(running_compiled_code=True)\n",
      "  File \"C:\\Users\\roeym\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2066, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(etype,\n",
      "  File \"C:\\Users\\roeym\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1367, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"C:\\Users\\roeym\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1267, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"C:\\Users\\roeym\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1124, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"C:\\Users\\roeym\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1082, in format_exception_as_a_whole\n",
      "    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n",
      "  File \"C:\\Users\\roeym\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 382, in find_recursion\n",
      "    return len(records), 0\n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\roeym\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2064, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\roeym\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"C:\\Users\\roeym\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 248, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\roeym\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 281, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"C:\\Users\\roeym\\anaconda3\\lib\\inspect.py\", line 1541, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"C:\\Users\\roeym\\anaconda3\\lib\\inspect.py\", line 1499, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"C:\\Users\\roeym\\anaconda3\\lib\\inspect.py\", line 709, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"C:\\Users\\roeym\\anaconda3\\lib\\inspect.py\", line 752, in getmodule\n",
      "    f = getabsfile(module)\n",
      "  File \"C:\\Users\\roeym\\anaconda3\\lib\\inspect.py\", line 721, in getabsfile\n",
      "    _filename = getsourcefile(object) or getfile(object)\n",
      "  File \"C:\\Users\\roeym\\anaconda3\\lib\\inspect.py\", line 706, in getsourcefile\n",
      "    if os.path.exists(filename):\n",
      "  File \"C:\\Users\\roeym\\anaconda3\\lib\\genericpath.py\", line 19, in exists\n",
      "    os.stat(path)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_22440/223147076.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 145\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_22440/223147076.py\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 134\u001b[1;33m     \u001b[0mavg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    135\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mavg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_22440/223147076.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, criterion, optimizer, train_loader, device)\u001b[0m\n\u001b[0;32m     72\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    395\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 396\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    397\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    172\u001b[0m     \u001b[1;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 173\u001b[1;33m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[0;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[1;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[0;32m   2063\u001b[0m                         \u001b[1;31m# in the engines. This should return a list of strings.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2064\u001b[1;33m                         \u001b[0mstb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2065\u001b[0m                     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'KeyboardInterrupt' object has no attribute '_render_traceback_'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mrun_ast_nodes\u001b[1;34m(self, nodelist, cell_name, interactivity, compiler, result)\u001b[0m\n\u001b[0;32m   3363\u001b[0m                         \u001b[0masy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3364\u001b[1;33m                     \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_code\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0masync_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0masy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3365\u001b[0m                         \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[1;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[0;32m   2065\u001b[0m                     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2066\u001b[1;33m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001b[0m\u001b[0;32m   2067\u001b[0m                                             value, tb, tb_offset=tb_offset)\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1366\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1367\u001b[1;33m         return FormattedTB.structured_traceback(\n\u001b[0m\u001b[0;32m   1368\u001b[0m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1266\u001b[0m             \u001b[1;31m# Verbose modes need a full traceback\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1267\u001b[1;33m             return VerboseTB.structured_traceback(\n\u001b[0m\u001b[0;32m   1268\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0metype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1123\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1124\u001b[1;33m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[0m\u001b[0;32m   1125\u001b[0m                                                                tb_offset)\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mformat_exception_as_a_whole\u001b[1;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[0;32m   1081\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1082\u001b[1;33m         \u001b[0mlast_unique\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfind_recursion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morig_etype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1083\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mfind_recursion\u001b[1;34m(etype, value, records)\u001b[0m\n\u001b[0;32m    381\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_recursion_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0metype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 382\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    383\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'NoneType' has no len()",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[1;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[0;32m   2063\u001b[0m                         \u001b[1;31m# in the engines. This should return a list of strings.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2064\u001b[1;33m                         \u001b[0mstb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2065\u001b[0m                     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'TypeError' object has no attribute '_render_traceback_'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\IPython\\core\\async_helpers.py\u001b[0m in \u001b[0;36m_pseudo_sync_runner\u001b[1;34m(coro)\u001b[0m\n\u001b[0;32m     66\u001b[0m     \"\"\"\n\u001b[0;32m     67\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m         \u001b[0mcoro\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_async\u001b[1;34m(self, raw_cell, store_history, silent, shell_futures, transformed_cell, preprocessing_exc_tuple)\u001b[0m\n\u001b[0;32m   3170\u001b[0m                     \u001b[0minteractivity\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'async'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3171\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3172\u001b[1;33m                 has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n\u001b[0m\u001b[0;32m   3173\u001b[0m                        interactivity=interactivity, compiler=compiler, result=result)\n\u001b[0;32m   3174\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mrun_ast_nodes\u001b[1;34m(self, nodelist, cell_name, interactivity, compiler, result)\u001b[0m\n\u001b[0;32m   3381\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3382\u001b[0m                 \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror_before_exec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3383\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshowtraceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3384\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3385\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[1;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[0;32m   2064\u001b[0m                         \u001b[0mstb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2065\u001b[0m                     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2066\u001b[1;33m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001b[0m\u001b[0;32m   2067\u001b[0m                                             value, tb, tb_offset=tb_offset)\n\u001b[0;32m   2068\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1365\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1366\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1367\u001b[1;33m         return FormattedTB.structured_traceback(\n\u001b[0m\u001b[0;32m   1368\u001b[0m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[0;32m   1369\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1265\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose_modes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1266\u001b[0m             \u001b[1;31m# Verbose modes need a full traceback\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1267\u001b[1;33m             return VerboseTB.structured_traceback(\n\u001b[0m\u001b[0;32m   1268\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0metype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1269\u001b[0m             )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1140\u001b[0m         \u001b[0mchained_exc_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1141\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[0mevalue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1142\u001b[1;33m             formatted_exceptions += self.format_exception_as_a_whole(etype, evalue, etb, lines_of_context,\n\u001b[0m\u001b[0;32m   1143\u001b[0m                                                                      chained_exceptions_tb_offset)\n\u001b[0;32m   1144\u001b[0m             \u001b[0mexception\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_parts_of_chained_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mformat_exception_as_a_whole\u001b[1;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[0;32m   1080\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1081\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1082\u001b[1;33m         \u001b[0mlast_unique\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfind_recursion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morig_etype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1083\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1084\u001b[0m         \u001b[0mframes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat_records\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlast_unique\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mfind_recursion\u001b[1;34m(etype, value, records)\u001b[0m\n\u001b[0;32m    380\u001b[0m     \u001b[1;31m# first frame (from in to out) that looks different.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    381\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_recursion_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0metype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 382\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    383\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m     \u001b[1;31m# Select filename, lineno, func_name to track frames with\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "from CNN2D import DNA_CNN\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch\n",
    "from sklearn.metrics import classification_report\n",
    "from tqdm import tqdm\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    data_path = \"./data/processed_data.npz\"\n",
    "\n",
    "    if os.path.exists(data_path):\n",
    "        # Load the sequences and labels from the .npz file if it already exists\n",
    "        data = np.load(data_path, allow_pickle=True)\n",
    "        sequences = torch.Tensor(data['sequences'])\n",
    "        labels = torch.LongTensor(data['labels'])\n",
    "    else:\n",
    "        sequences = []\n",
    "        labels = []\n",
    "        reduced_data = pd.read_csv(\"./data/reduced_data_encoded.csv\")\n",
    "        reduced_data[\"id\"] = reduced_data[\"id\"].str.strip('\"')\n",
    "        npy_files = os.listdir(\"./data/encoded_sequences/\")\n",
    "        for npy_file in npy_files:\n",
    "            id = npy_file.split(\".\")[0]\n",
    "            sequence = np.load(f\"./data/encoded_sequences/{npy_file}\")\n",
    "            label = reduced_data.loc[reduced_data[\"id\"] == id, \"label\"].values\n",
    "            sequences.append(sequence)\n",
    "            labels.append(label)\n",
    "\n",
    "        # Find the maximum and minimum sequence lengths\n",
    "        seq_lengths = [len(seq) for seq in sequences]\n",
    "        max_length = max(seq_lengths)\n",
    "        min_length = min(seq_lengths)\n",
    "        target_length = (max_length + min_length) // 2\n",
    "\n",
    "        # Pad or truncate sequences to match the target length\n",
    "        for i in range(len(sequences)):\n",
    "            difference = target_length - len(sequences[i])\n",
    "            if difference > 0:  # Sequence is shorter than target length\n",
    "                padding = np.repeat([[0.25, 0.25, 0.25, 0.25]], difference, axis=0)\n",
    "                sequences[i] = np.concatenate([sequences[i], padding])\n",
    "            elif difference < 0:  # Sequence is longer than target length\n",
    "                sequences[i] = sequences[i][:target_length]\n",
    "\n",
    "        # Save the processed sequences and labels to a .npz file\n",
    "        np.savez(data_path, sequences=sequences, labels=labels)\n",
    "\n",
    "        sequences = torch.Tensor(sequences)\n",
    "        labels = torch.LongTensor(labels)\n",
    "\n",
    "    return sequences, labels\n",
    "\n",
    "def train(model, criterion, optimizer, train_loader, device):\n",
    "    model.train()  # set the model to training mode\n",
    "    total_loss = 0.0\n",
    "    for i, (sequences, labels) in enumerate(tqdm(train_loader)):\n",
    "        sequences = sequences.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(sequences)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "\n",
    "def evaluate(model, test_loader,device):\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for sequences, labels in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "            sequences, labels = sequences.to(device), labels.to(device)\n",
    "            outputs = model(sequences)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Test Accuracy of the model on the test sequences: {} %'.format((correct / total) * 100))\n",
    "    return torch.tensor(all_predictions), torch.tensor(all_labels)\n",
    "\n",
    "\n",
    "def main():\n",
    "    data = np.load(\"./data/processed_data.npz\", allow_pickle=True)\n",
    "    sequences = torch.Tensor(data['sequences'])\n",
    "    labels = torch.LongTensor(data['labels'])\n",
    "    print(sequences.shape)\n",
    "    print(labels.shape)\n",
    "\n",
    "    sequences = sequences.unsqueeze(1)\n",
    "    labels = labels.squeeze()\n",
    "\n",
    "    print(sequences.shape)\n",
    "    print(labels.shape)\n",
    "\n",
    "    # sequences, labels = load_data()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(sequences, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "    train_data = TensorDataset(X_train, y_train)\n",
    "    train_loader = DataLoader(train_data, batch_size=32)\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    if device.type == 'cuda':\n",
    "        print('Running on GPU: ', torch.cuda.get_device_name(0))  # Print the name of the GPU\n",
    "    else:\n",
    "        print(\"Running on CPU\")\n",
    "\n",
    "    model = DNA_CNN()\n",
    "    model.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    avg = train(model, criterion, optimizer, train_loader, device)\n",
    "    print(avg)\n",
    "\n",
    "    test_data = TensorDataset(X_test, y_test)\n",
    "    test_loader = DataLoader(test_data, batch_size=32)\n",
    "    predicted, test_labels = evaluate(model, test_loader,device)\n",
    "\n",
    "    print(classification_report(test_labels.cpu().numpy(), predicted.cpu().numpy()))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a63e1fb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80099d65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2cd125ff",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cuda'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_22440/3609991021.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mcuda\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'cuda'"
     ]
    }
   ],
   "source": [
    "import cuda\n",
    "print(cuda.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb35203f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
